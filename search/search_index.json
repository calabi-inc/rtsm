{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"RTSM \u2014 Real-Time Spatio-Semantic Memory","text":"<p>Object-centric queryable memory for spatial AI and robotics.</p> <p>RTSM builds a persistent, searchable memory of objects in 3D space from RGB-D camera streams. Ask natural language queries like \"Where is the red mug?\" and get answers grounded in real-world coordinates.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Real-time segmentation \u2014 FastSAM extracts object instances from each frame</li> <li>Semantic embeddings \u2014 CLIP encodes visual features for natural language queries</li> <li>Persistent memory \u2014 Objects are tracked across views, fused, and promoted to long-term memory</li> <li>Spatial indexing \u2014 Fast proximity queries via 3D grid + vector search (FAISS)</li> <li>Queryable \u2014 REST API and semantic search: find objects by description</li> </ul> <pre><code>// \"Where is the red backpack?\"\n{ \"id\": \"a3f2c1\", \"xyz\": [1.2, 0.4, 2.1], \"confidence\": 0.87 }\n</code></pre>"},{"location":"#quick-links","title":"Quick Links","text":"- :material-download: **[Installation](getting-started/installation.md)** \u2014 Get RTSM running - :material-rocket-launch: **[Quick Start](getting-started/quick-start.md)** \u2014 Your first query in 5 minutes - :material-cog: **[Configuration](getting-started/configuration.md)** \u2014 Tune for your setup - :material-api: **[REST API](api/rest-api.md)** \u2014 API reference"},{"location":"#performance","title":"Performance","text":"<p>Benchmarks on RTX 5090:</p> Stage Metric Input throttling 30 Hz raw \u2192 5\u20137 Hz processed Proto-object yield &gt;90% of static masks accumulate Frame latency &lt;30 ms (FastSAM + CLIP) LTM upsert rate 5 s default interval"},{"location":"#license","title":"License","text":"<p>Apache-2.0 \u2014 See GitHub for details.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thanks for your interest in contributing to RTSM! See our Contributing Guide for details on:</p> <ul> <li>Getting started</li> <li>Development setup</li> <li>Commit message format</li> <li>Pull request process</li> <li>Ideas for contributions</li> </ul>"},{"location":"contributing/#quick-links","title":"Quick Links","text":"<ul> <li>GitHub Issues \u2014 Report bugs or request features</li> <li>GitHub Discussions \u2014 Ask questions</li> <li>Calabi \u2014 Collaboration inquiries</li> </ul>"},{"location":"api/rest-api/","title":"REST API Reference","text":"<p>RTSM exposes a REST API for querying objects and system state.</p> <p>Base URL: <code>http://localhost:8000</code></p>"},{"location":"api/rest-api/#objects","title":"Objects","text":""},{"location":"api/rest-api/#list-all-objects","title":"List All Objects","text":"<pre><code>GET /objects\n</code></pre> <p>Response:</p> <pre><code>[\n  {\n    \"id\": \"a3f2c1\",\n    \"label\": \"backpack\",\n    \"xyz\": [1.2, 0.4, 2.1],\n    \"confidence\": 0.87,\n    \"hits\": 15,\n    \"confirmed\": true,\n    \"last_seen\": \"2024-01-15T10:30:00Z\"\n  }\n]\n</code></pre> <p>Query Parameters:</p> Parameter Type Description <code>confirmed_only</code> bool Only return confirmed objects (default: true) <code>label</code> string Filter by label <code>limit</code> int Max results (default: 100)"},{"location":"api/rest-api/#get-object-by-id","title":"Get Object by ID","text":"<pre><code>GET /objects/{id}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"id\": \"a3f2c1\",\n  \"label\": \"backpack\",\n  \"xyz\": [1.2, 0.4, 2.1],\n  \"confidence\": 0.87,\n  \"hits\": 15,\n  \"confirmed\": true,\n  \"stability\": 0.82,\n  \"view_count\": 4,\n  \"last_seen\": \"2024-01-15T10:30:00Z\"\n}\n</code></pre>"},{"location":"api/rest-api/#get-object-image","title":"Get Object Image","text":"<pre><code>GET /objects/{id}/image\n</code></pre> <p>Returns the best JPEG crop of the object.</p> <p>Response: <code>image/jpeg</code></p>"},{"location":"api/rest-api/#search","title":"Search","text":""},{"location":"api/rest-api/#semantic-search","title":"Semantic Search","text":"<pre><code>GET /search/semantic?query={text}&amp;top_k={n}\n</code></pre> <p>Parameters:</p> Parameter Type Description <code>query</code> string Natural language query <code>top_k</code> int Number of results (default: 5) <p>Example:</p> <pre><code>curl \"http://localhost:8000/search/semantic?query=red%20mug&amp;top_k=5\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"query\": \"red mug\",\n  \"results\": [\n    {\n      \"id\": \"b7d4e2\",\n      \"label\": \"mug\",\n      \"xyz\": [0.8, 0.2, 1.5],\n      \"score\": 0.82\n    },\n    {\n      \"id\": \"c9f1a3\",\n      \"label\": \"cup\",\n      \"xyz\": [1.1, 0.3, 1.8],\n      \"score\": 0.71\n    }\n  ]\n}\n</code></pre>"},{"location":"api/rest-api/#spatial-search","title":"Spatial Search","text":"<pre><code>GET /search/spatial?x={x}&amp;y={y}&amp;z={z}&amp;radius={r}\n</code></pre> <p>Find objects within a radius of a point.</p> <p>Parameters:</p> Parameter Type Description <code>x</code>, <code>y</code>, <code>z</code> float Center point (meters) <code>radius</code> float Search radius (meters) <p>Response:</p> <pre><code>{\n  \"center\": [1.0, 0.5, 2.0],\n  \"radius\": 0.5,\n  \"results\": [\n    {\n      \"id\": \"a3f2c1\",\n      \"label\": \"backpack\",\n      \"xyz\": [1.2, 0.4, 2.1],\n      \"distance\": 0.24\n    }\n  ]\n}\n</code></pre>"},{"location":"api/rest-api/#system","title":"System","text":""},{"location":"api/rest-api/#health-check","title":"Health Check","text":"<pre><code>GET /health\n</code></pre> <p>Response:</p> <pre><code>{\n  \"status\": \"ok\",\n  \"uptime\": 3600\n}\n</code></pre>"},{"location":"api/rest-api/#statistics","title":"Statistics","text":"<pre><code>GET /stats/detailed\n</code></pre> <p>Response:</p> <pre><code>{\n  \"frames_processed\": 12450,\n  \"objects\": {\n    \"total\": 47,\n    \"confirmed\": 32,\n    \"proto\": 15\n  },\n  \"memory\": {\n    \"working_memory_mb\": 128,\n    \"ltm_vectors\": 32\n  },\n  \"performance\": {\n    \"avg_frame_ms\": 28.5,\n    \"fps\": 6.2\n  }\n}\n</code></pre>"},{"location":"api/rest-api/#error-responses","title":"Error Responses","text":"<p>All errors follow this format:</p> <pre><code>{\n  \"error\": \"not_found\",\n  \"message\": \"Object with ID 'xyz' not found\"\n}\n</code></pre> Status Meaning 400 Bad request (invalid parameters) 404 Object not found 500 Internal server error"},{"location":"api/rest-api/#next-steps","title":"Next Steps","text":"<ul> <li>WebSocket API \u2014 Real-time streaming</li> <li>Quick Start \u2014 Try these endpoints</li> </ul>"},{"location":"api/websocket/","title":"WebSocket API","text":"<p>RTSM provides a WebSocket endpoint for real-time streaming of point clouds and object updates.</p> <p>Endpoint: <code>ws://localhost:8081</code></p>"},{"location":"api/websocket/#connection","title":"Connection","text":"<pre><code>const ws = new WebSocket('ws://localhost:8081');\n\nws.onopen = () =&gt; {\n  console.log('Connected to RTSM');\n};\n\nws.onmessage = (event) =&gt; {\n  const data = JSON.parse(event.data);\n  handleMessage(data);\n};\n</code></pre>"},{"location":"api/websocket/#message-types","title":"Message Types","text":""},{"location":"api/websocket/#point-cloud-update","title":"Point Cloud Update","text":"<p>Streamed periodically with the current point cloud.</p> <pre><code>{\n  \"type\": \"point_cloud\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"points\": [\n    [1.2, 0.4, 2.1, 255, 128, 64],\n    [1.3, 0.5, 2.0, 240, 120, 60]\n  ],\n  \"count\": 10000\n}\n</code></pre> <p>Each point is <code>[x, y, z, r, g, b]</code>.</p>"},{"location":"api/websocket/#objects-update","title":"Objects Update","text":"<p>Sent when objects are created, updated, or removed.</p> <pre><code>{\n  \"type\": \"objects_update\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"objects\": [\n    {\n      \"id\": \"a3f2c1\",\n      \"label\": \"backpack\",\n      \"xyz\": [1.2, 0.4, 2.1],\n      \"confidence\": 0.87,\n      \"confirmed\": true\n    }\n  ]\n}\n</code></pre>"},{"location":"api/websocket/#object-created","title":"Object Created","text":"<p>Sent when a new object is first detected.</p> <pre><code>{\n  \"type\": \"object_created\",\n  \"object\": {\n    \"id\": \"b7d4e2\",\n    \"label\": \"mug\",\n    \"xyz\": [0.8, 0.2, 1.5],\n    \"confidence\": 0.65,\n    \"confirmed\": false\n  }\n}\n</code></pre>"},{"location":"api/websocket/#object-confirmed","title":"Object Confirmed","text":"<p>Sent when a proto-object is promoted to confirmed status.</p> <pre><code>{\n  \"type\": \"object_confirmed\",\n  \"object\": {\n    \"id\": \"b7d4e2\",\n    \"label\": \"mug\",\n    \"xyz\": [0.8, 0.2, 1.5],\n    \"confidence\": 0.82,\n    \"confirmed\": true\n  }\n}\n</code></pre>"},{"location":"api/websocket/#system-stats","title":"System Stats","text":"<p>Periodic system statistics.</p> <pre><code>{\n  \"type\": \"stats\",\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"fps\": 6.2,\n  \"objects_count\": 32,\n  \"frame_latency_ms\": 28\n}\n</code></pre>"},{"location":"api/websocket/#client-commands","title":"Client Commands","text":"<p>Send JSON commands to control the stream:</p>"},{"location":"api/websocket/#subscribe-to-specific-events","title":"Subscribe to Specific Events","text":"<pre><code>{\n  \"command\": \"subscribe\",\n  \"events\": [\"objects_update\", \"stats\"]\n}\n</code></pre>"},{"location":"api/websocket/#set-point-cloud-decimation","title":"Set Point Cloud Decimation","text":"<p>Reduce point cloud density for bandwidth:</p> <pre><code>{\n  \"command\": \"set_decimation\",\n  \"factor\": 4\n}\n</code></pre>"},{"location":"api/websocket/#pauseresume-streaming","title":"Pause/Resume Streaming","text":"<pre><code>{\n  \"command\": \"pause\"\n}\n</code></pre> <pre><code>{\n  \"command\": \"resume\"\n}\n</code></pre>"},{"location":"api/websocket/#example-threejs-integration","title":"Example: Three.js Integration","text":"<pre><code>const ws = new WebSocket('ws://localhost:8081');\n\nws.onmessage = (event) =&gt; {\n  const data = JSON.parse(event.data);\n\n  if (data.type === 'point_cloud') {\n    updatePointCloud(data.points);\n  }\n\n  if (data.type === 'objects_update') {\n    updateObjectMarkers(data.objects);\n  }\n};\n\nfunction updatePointCloud(points) {\n  const geometry = new THREE.BufferGeometry();\n  const positions = new Float32Array(points.length * 3);\n  const colors = new Float32Array(points.length * 3);\n\n  points.forEach((p, i) =&gt; {\n    positions[i * 3] = p[0];\n    positions[i * 3 + 1] = p[1];\n    positions[i * 3 + 2] = p[2];\n    colors[i * 3] = p[3] / 255;\n    colors[i * 3 + 1] = p[4] / 255;\n    colors[i * 3 + 2] = p[5] / 255;\n  });\n\n  geometry.setAttribute('position', new THREE.BufferAttribute(positions, 3));\n  geometry.setAttribute('color', new THREE.BufferAttribute(colors, 3));\n\n  // Update your scene...\n}\n</code></pre>"},{"location":"api/websocket/#next-steps","title":"Next Steps","text":"<ul> <li>REST API \u2014 Query endpoints</li> <li>3D Demo source \u2014 Full Three.js example</li> </ul>"},{"location":"concepts/architecture/","title":"Architecture","text":"<p>RTSM processes RGB-D frames through a pipeline that extracts, tracks, and stores objects in a queryable spatial memory.</p>"},{"location":"concepts/architecture/#system-overview","title":"System Overview","text":"<pre><code>RGB-D Sensor + SLAM (Pose)\n         |\n         | ZeroMQ\n         v\n    I/O Layer\n    ZMQ Bridge -&gt; IngestQueue -&gt; FramePacket\n         |\n         v\n  Perception Pipeline\n    FastSAM -&gt; Mask Staging -&gt; Top-K Select -&gt; CLIP Encode -&gt; Vocab Classify\n         |\n         v\n   Association\n    Proximity Query -&gt; Embedding Cosine Sim -&gt; Score Fusion (match/create)\n         |\n         v\n      Memory\n    Working Memory -&gt; Long-Term Memory (FAISS)\n         |\n         v\n  API &amp; Visualization\n    REST API | WebSocket | 3D Demo\n</code></pre>"},{"location":"concepts/architecture/#components","title":"Components","text":""},{"location":"concepts/architecture/#io-layer","title":"I/O Layer","text":"<p>Receives RGB-D frames and camera poses via ZeroMQ. Frames are buffered in an ingest queue with keyframe gating to throttle processing (30 Hz \u2192 5-7 Hz).</p>"},{"location":"concepts/architecture/#perception-pipeline","title":"Perception Pipeline","text":"<ol> <li>FastSAM \u2014 Segments the RGB image into object masks</li> <li>Mask Staging \u2014 Filters by area (rejects too small/large)</li> <li>Top-K Select \u2014 Limits masks per frame for processing budget</li> <li>CLIP Encode \u2014 Extracts 512-dim embedding from each mask crop</li> <li>Vocab Classify \u2014 Assigns labels via cosine similarity to text embeddings</li> </ol>"},{"location":"concepts/architecture/#association","title":"Association","text":"<p>Matches new observations to existing objects:</p> <ol> <li>Proximity Query \u2014 Find nearby objects in 3D space</li> <li>Embedding Similarity \u2014 Compare CLIP vectors</li> <li>Score Fusion \u2014 Weighted combination \u2192 match or create new</li> </ol>"},{"location":"concepts/architecture/#memory","title":"Memory","text":"<ul> <li>Working Memory \u2014 Active object states (position, embeddings, view history)</li> <li>Long-Term Memory \u2014 Confirmed objects indexed in FAISS for semantic search</li> </ul>"},{"location":"concepts/architecture/#api-layer","title":"API Layer","text":"<ul> <li>REST API \u2014 Query objects, search by text, get stats</li> <li>WebSocket \u2014 Stream point clouds and object updates</li> <li>3D Demo \u2014 Three.js visualization</li> </ul>"},{"location":"concepts/architecture/#data-flow","title":"Data Flow","text":"<pre><code>Frame \u2192 Segment \u2192 Encode \u2192 Associate \u2192 Update Memory \u2192 Index \u2192 Query\n</code></pre> <p>Each frame takes &lt;30ms end-to-end on RTX 5090.</p>"},{"location":"concepts/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Perception Pipeline \u2014 Deep dive into segmentation and encoding</li> <li>Memory Model \u2014 How objects are tracked and promoted</li> </ul>"},{"location":"concepts/memory-model/","title":"Memory Model","text":"<p>RTSM maintains a two-tier memory system: Working Memory for active tracking and Long-Term Memory for persistent, queryable storage.</p>"},{"location":"concepts/memory-model/#object-lifecycle","title":"Object Lifecycle","text":"<pre><code>Observation \u2192 Proto-Object \u2192 Confirmed Object \u2192 Long-Term Memory\n</code></pre>"},{"location":"concepts/memory-model/#working-memory","title":"Working Memory","text":"<p>Working memory holds <code>ObjectState</code> records for all tracked objects:</p> <pre><code>ObjectState:\n  id: str                    # unique identifier\n  xyz_world: [x, y, z]       # 3D position in world frame\n  emb_mean: [512]            # running mean of CLIP embeddings\n  emb_gallery: [[512], ...]  # gallery of view embeddings\n  view_bins: set             # viewpoint directions seen\n  label_scores: dict         # EWMA label confidences\n  stability: float           # embedding consistency score\n  hits: int                  # observation count\n  confirmed: bool            # promotion status\n  image_crops: [bytes, ...]  # JPEG snapshots\n  last_seen: timestamp\n</code></pre>"},{"location":"concepts/memory-model/#proto-objects","title":"Proto-Objects","text":"<p>New observations create proto-objects \u2014 tentative entries that may be noise or transient.</p>"},{"location":"concepts/memory-model/#promotion-criteria","title":"Promotion Criteria","text":"<p>Proto-objects become confirmed when:</p> Criterion Default <code>hits</code> \u2265 2 <code>stability</code> \u2265 0.5 <code>view_bins</code> \u2265 2 viewpoints <p>This filters out:</p> <ul> <li>Single-frame noise</li> <li>Inconsistent detections</li> <li>Objects seen from only one angle</li> </ul>"},{"location":"concepts/memory-model/#association","title":"Association","text":"<p>When a new observation arrives, we find the best matching object:</p> <ol> <li>Spatial proximity \u2014 Query objects within radius (default: 0.3m)</li> <li>Embedding similarity \u2014 Cosine similarity of CLIP vectors</li> <li>Score fusion \u2014 Weighted combination</li> </ol> <pre><code>score = \u03b1 * spatial_score + (1-\u03b1) * embedding_score\n\nif score &gt; threshold:\n    match \u2192 update existing object\nelse:\n    create new proto-object\n</code></pre>"},{"location":"concepts/memory-model/#update-on-match","title":"Update on Match","text":"<p>When matched, the object state is updated:</p> <ul> <li>Position: Exponential moving average</li> <li>Embedding: Added to gallery, mean updated</li> <li>Hits: Incremented</li> <li>Stability: Recalculated from embedding variance</li> </ul>"},{"location":"concepts/memory-model/#long-term-memory","title":"Long-Term Memory","text":"<p>Confirmed objects are periodically upserted to Long-Term Memory (default: every 5 seconds).</p>"},{"location":"concepts/memory-model/#storage","title":"Storage","text":"<ul> <li>FAISS (default) \u2014 Local vector index</li> <li>Milvus (optional) \u2014 Distributed vector database</li> </ul>"},{"location":"concepts/memory-model/#indexed-fields","title":"Indexed Fields","text":"Field Purpose <code>emb_mean</code> Semantic search <code>xyz_world</code> Spatial queries <code>label</code> Filtered search"},{"location":"concepts/memory-model/#semantic-search","title":"Semantic Search","text":"<p>Text queries are encoded via CLIP and matched against stored embeddings:</p> <pre><code>\"red mug\" \u2192 CLIP \u2192 query vector \u2192 FAISS top-k \u2192 ranked objects\n</code></pre>"},{"location":"concepts/memory-model/#expiry","title":"Expiry","text":"<p>Objects not seen for a configurable duration are:</p> <ol> <li>Marked as stale in working memory</li> <li>Retained in long-term memory (queryable but flagged)</li> </ol> <p>This handles:</p> <ul> <li>Objects that moved</li> <li>Objects removed from scene</li> <li>Temporary occlusions</li> </ul>"},{"location":"concepts/memory-model/#next-steps","title":"Next Steps","text":"<ul> <li>REST API \u2014 Query the memory</li> <li>Architecture \u2014 Full system overview</li> </ul>"},{"location":"concepts/perception-pipeline/","title":"Perception Pipeline","text":"<p>The perception pipeline extracts object instances from RGB-D frames and encodes them for matching and search.</p>"},{"location":"concepts/perception-pipeline/#pipeline-stages","title":"Pipeline Stages","text":"<pre><code>RGB Frame \u2192 FastSAM \u2192 Mask Filter \u2192 CLIP Encode \u2192 Vocab Classify\n</code></pre>"},{"location":"concepts/perception-pipeline/#1-segmentation-fastsam","title":"1. Segmentation (FastSAM)","text":"<p>FastSAM generates instance masks from the RGB image.</p> <ul> <li>Input: 640\u00d7480 RGB image</li> <li>Output: Variable number of binary masks</li> <li>Speed: ~10-15ms per frame</li> </ul> <p>FastSAM is a CNN-based approximation of SAM (Segment Anything), trading some accuracy for 50\u00d7 faster inference.</p>"},{"location":"concepts/perception-pipeline/#2-mask-filtering","title":"2. Mask Filtering","text":"<p>Heuristic filters remove unsuitable masks:</p> Filter Purpose Min area (0.1%) Remove noise/tiny fragments Max area (50%) Remove walls/floors/background Aspect ratio Remove extreme shapes Edge touching Optionally filter partial objects <p>This typically rejects 10-15% of masks as insignificant.</p>"},{"location":"concepts/perception-pipeline/#3-top-k-selection","title":"3. Top-K Selection","text":"<p>After filtering, we keep only the top K masks (default: 20) per frame to bound compute cost. Selection prioritizes:</p> <ol> <li>Mask confidence score</li> <li>Area (medium-sized preferred)</li> <li>Distance from frame center</li> </ol>"},{"location":"concepts/perception-pipeline/#4-clip-encoding","title":"4. CLIP Encoding","text":"<p>Each mask is:</p> <ol> <li>Cropped from the RGB image (with padding)</li> <li>Resized to 224\u00d7224</li> <li>Encoded via CLIP ViT-B/32</li> </ol> <p>Output: 512-dimensional embedding vector</p> <p>These embeddings enable:</p> <ul> <li>Matching observations across frames</li> <li>Semantic search via text queries</li> </ul>"},{"location":"concepts/perception-pipeline/#5-vocabulary-classification","title":"5. Vocabulary Classification","text":"<p>Object labels are assigned by comparing the CLIP embedding to pre-computed text embeddings:</p> <pre><code>text_embeddings = clip.encode_text([\n    \"a photo of a mug\",\n    \"a photo of a backpack\",\n    \"a photo of a chair\",\n    ...\n])\n\nsimilarities = cosine_similarity(image_embedding, text_embeddings)\nlabel = vocab[argmax(similarities)]\nconfidence = max(similarities)\n</code></pre> <p>The vocabulary is configurable \u2014 add domain-specific objects for your use case.</p>"},{"location":"concepts/perception-pipeline/#performance","title":"Performance","text":"Stage Time (RTX 5090) FastSAM ~12ms Mask filtering &lt;1ms CLIP encode (20 masks) ~15ms Vocab classify &lt;1ms Total &lt;30ms"},{"location":"concepts/perception-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Memory Model \u2014 How observations become persistent objects</li> <li>Architecture \u2014 Full system overview</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>RTSM is configured via <code>config/rtsm.yaml</code>. This page covers the main settings.</p>"},{"location":"getting-started/configuration/#configuration-file","title":"Configuration File","text":"<pre><code># config/rtsm.yaml\n\ncamera:\n  width: 640\n  height: 480\n  fx: 615.0  # focal length x\n  fy: 615.0  # focal length y\n  cx: 320.0  # principal point x\n  cy: 240.0  # principal point y\n\nio:\n  zmq_camera_addr: \"tcp://localhost:5555\"\n  zmq_slam_addr: \"tcp://localhost:5556\"\n\npipeline:\n  # Mask filtering\n  min_mask_area: 0.001    # min area as fraction of image\n  max_mask_area: 0.5      # max area as fraction of image\n  top_k_masks: 20         # max masks per frame\n\n  # Association thresholds\n  proximity_radius: 0.3   # meters\n  embedding_threshold: 0.7\n\nmemory:\n  # Object promotion\n  min_hits: 2\n  min_stability: 0.5\n  min_views: 2\n\n  # Long-term memory\n  ltm_upsert_interval: 5.0  # seconds\n  vector_store: \"faiss\"     # or \"milvus\"\n\napi:\n  host: \"0.0.0.0\"\n  port: 8000\n\nvisualization:\n  ws_port: 8081\n</code></pre>"},{"location":"getting-started/configuration/#key-settings","title":"Key Settings","text":""},{"location":"getting-started/configuration/#camera-intrinsics","title":"Camera Intrinsics","text":"<p>Match these to your RGB-D camera. For RealSense D435i:</p> <pre><code>camera:\n  width: 640\n  height: 480\n  fx: 615.0\n  fy: 615.0\n  cx: 320.0\n  cy: 240.0\n</code></pre>"},{"location":"getting-started/configuration/#io-endpoints","title":"I/O Endpoints","text":"<p>Configure ZeroMQ addresses for your data sources:</p> <pre><code>io:\n  zmq_camera_addr: \"tcp://localhost:5555\"  # RGB-D frames\n  zmq_slam_addr: \"tcp://localhost:5556\"    # Pose estimates\n</code></pre>"},{"location":"getting-started/configuration/#pipeline-tuning","title":"Pipeline Tuning","text":"<p>Adjust mask filtering for your environment:</p> <pre><code>pipeline:\n  min_mask_area: 0.001  # filter tiny noise\n  max_mask_area: 0.5    # filter wall-sized masks\n  top_k_masks: 20       # limit processing per frame\n</code></pre>"},{"location":"getting-started/configuration/#memory-settings","title":"Memory Settings","text":"<p>Control when proto-objects become confirmed:</p> <pre><code>memory:\n  min_hits: 2        # seen at least twice\n  min_stability: 0.5 # embedding consistency\n  min_views: 2       # from multiple viewpoints\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Some settings can be overridden via environment:</p> <pre><code>export RTSM_API_PORT=9000\nexport RTSM_LOG_LEVEL=DEBUG\n</code></pre>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture \u2014 Understand the system design</li> <li>REST API \u2014 API reference</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>CUDA-capable GPU (tested on RTX 3080, RTX 5090)</li> <li>RGB-D camera (Intel RealSense D435i tested)</li> <li>SLAM system providing poses (RTAB-Map, ORB-SLAM3)</li> </ul> <p>WSL2 Users</p> <p>Tested on WSL2 Ubuntu 22.04 with RTAB-Map. WSL2 has USB passthrough limitations \u2014 you may need usbipd-win for camera access.</p>"},{"location":"getting-started/installation/#install-rtsm","title":"Install RTSM","text":"<pre><code>git clone https://github.com/calabi-inc/rtsm.git\ncd rtsm\n# Install dependencies\npip install -r requirements.txt\n\n# Install RTSM in editable mode\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#download-models","title":"Download Models","text":""},{"location":"getting-started/installation/#fastsam-weights","title":"FastSAM Weights","text":"<pre><code>mkdir -p model_store/fastsam\n# Download FastSAM-x.pt to model_store/fastsam/\n</code></pre> <p>Download from: FastSAM Releases</p>"},{"location":"getting-started/installation/#clip-weights","title":"CLIP Weights","text":"<p>CLIP weights are auto-downloaded on first run, or you can pre-fetch:</p> <pre><code>python scripts/fetch_clip.py\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>python -c \"import rtsm; print(rtsm.__version__)\"\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start \u2014 Run your first query</li> <li>Configuration \u2014 Customize for your setup</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This guide walks you through running RTSM and making your first semantic query.</p>"},{"location":"getting-started/quick-start/#1-start-rtsm","title":"1. Start RTSM","text":"<p>RTSM expects an RGB-D stream with poses via ZeroMQ. Start the main service:</p> <pre><code>python -m rtsm.run\n</code></pre> <p>This launches:</p> Service Address REST API <code>http://localhost:8000</code> WebSocket (visualization) <code>ws://localhost:8081</code>"},{"location":"getting-started/quick-start/#2-verify-its-running","title":"2. Verify It's Running","text":"<pre><code>curl http://localhost:8000/stats/detailed\n</code></pre> <p>You should see system stats including frame count, object count, and memory usage.</p>"},{"location":"getting-started/quick-start/#3-list-detected-objects","title":"3. List Detected Objects","text":"<p>Once frames are streaming, objects will appear in memory:</p> <pre><code>curl http://localhost:8000/objects\n</code></pre> <p>Response:</p> <pre><code>[\n  {\n    \"id\": \"a3f2c1\",\n    \"label\": \"backpack\",\n    \"xyz\": [1.2, 0.4, 2.1],\n    \"confidence\": 0.87\n  },\n  ...\n]\n</code></pre>"},{"location":"getting-started/quick-start/#4-semantic-search","title":"4. Semantic Search","text":"<p>Ask natural language queries:</p> <pre><code>curl \"http://localhost:8000/search/semantic?query=red%20mug&amp;top_k=5\"\n</code></pre> <p>Response:</p> <pre><code>{\n  \"query\": \"red mug\",\n  \"results\": [\n    {\n      \"id\": \"b7d4e2\",\n      \"label\": \"mug\",\n      \"xyz\": [0.8, 0.2, 1.5],\n      \"score\": 0.82\n    }\n  ]\n}\n</code></pre>"},{"location":"getting-started/quick-start/#5-view-in-3d-optional","title":"5. View in 3D (Optional)","text":"<p>Open the visualization demo in your browser:</p> <pre><code>http://localhost:8081\n</code></pre> <p>This shows a Three.js point cloud with detected objects overlaid.</p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration \u2014 Tune thresholds and endpoints</li> <li>REST API Reference \u2014 Full API documentation</li> <li>RTAB-Map Setup \u2014 Connect your SLAM system</li> </ul>"},{"location":"guides/realsense-setup/","title":"RealSense Setup","text":"<p>This guide covers setting up Intel RealSense D435i for use with RTSM.</p>"},{"location":"guides/realsense-setup/#hardware","title":"Hardware","text":"<p>Tested cameras:</p> <ul> <li>Intel RealSense D435i (recommended)</li> <li>Intel RealSense D435</li> <li>Intel RealSense D455</li> </ul> <p>The D435i includes an IMU, which can improve SLAM tracking.</p>"},{"location":"guides/realsense-setup/#installation","title":"Installation","text":""},{"location":"guides/realsense-setup/#ubuntu-wsl2","title":"Ubuntu / WSL2","text":"<pre><code># Add Intel repo\nsudo apt-key adv --keyserver keyserver.ubuntu.com --recv-key F6E65AC044F831AC80A06380C8B3A55A6F3EFCDE\nsudo add-apt-repository \"deb https://librealsense.intel.com/Debian/apt-repo $(lsb_release -cs) main\"\n\n# Install\nsudo apt update\nsudo apt install librealsense2-dkms librealsense2-utils librealsense2-dev\n</code></pre>"},{"location":"guides/realsense-setup/#python-sdk","title":"Python SDK","text":"<pre><code>pip install pyrealsense2\n</code></pre>"},{"location":"guides/realsense-setup/#verify-installation","title":"Verify Installation","text":"<pre><code>realsense-viewer\n</code></pre> <p>This should open the RealSense GUI showing RGB and depth streams.</p>"},{"location":"guides/realsense-setup/#running-the-scanner","title":"Running the Scanner","text":"<p>RTSM includes a RealSense capture utility:</p> <pre><code># From rtsm-realsense-scanner repo\npython realsense_scanner.py --zmq-pub tcp://*:5555\n</code></pre>"},{"location":"guides/realsense-setup/#options","title":"Options","text":"Flag Description <code>--width</code> Frame width (default: 640) <code>--height</code> Frame height (default: 480) <code>--fps</code> Frame rate (default: 30) <code>--zmq-pub</code> ZeroMQ publish address"},{"location":"guides/realsense-setup/#message-format","title":"Message Format","text":"<p>Published frames:</p> <pre><code>{\n  \"timestamp\": 1705312200.123,\n  \"frame_id\": 12345,\n  \"rgb\": \"&lt;base64 encoded JPEG&gt;\",\n  \"depth\": \"&lt;base64 encoded 16-bit PNG&gt;\",\n  \"intrinsics\": {\n    \"fx\": 615.0,\n    \"fy\": 615.0,\n    \"cx\": 320.0,\n    \"cy\": 240.0\n  }\n}\n</code></pre>"},{"location":"guides/realsense-setup/#configuration","title":"Configuration","text":""},{"location":"guides/realsense-setup/#camera-settings","title":"Camera Settings","text":"<p>For indoor use:</p> <pre><code>config = rs.config()\nconfig.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\nconfig.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n</code></pre>"},{"location":"guides/realsense-setup/#depth-filtering","title":"Depth Filtering","text":"<p>Enable post-processing for cleaner depth:</p> <pre><code>spatial = rs.spatial_filter()\nspatial.set_option(rs.option.filter_magnitude, 2)\nspatial.set_option(rs.option.filter_smooth_alpha, 0.5)\n\ntemporal = rs.temporal_filter()\nhole_filling = rs.hole_filling_filter()\n\ndepth_frame = spatial.process(depth_frame)\ndepth_frame = temporal.process(depth_frame)\ndepth_frame = hole_filling.process(depth_frame)\n</code></pre>"},{"location":"guides/realsense-setup/#align-depth-to-color","title":"Align Depth to Color","text":"<p>Important for correct projection:</p> <pre><code>align = rs.align(rs.stream.color)\naligned_frames = align.process(frames)\n</code></pre>"},{"location":"guides/realsense-setup/#wsl2-setup","title":"WSL2 Setup","text":"<p>WSL2 requires USB passthrough via usbipd-win.</p>"},{"location":"guides/realsense-setup/#windows-side-powershell-admin","title":"Windows Side (PowerShell Admin)","text":"<pre><code># Install usbipd\nwinget install usbipd\n\n# List devices\nusbipd wsl list\n\n# Attach RealSense (find the Bus ID from list)\nusbipd wsl attach --busid 2-3\n</code></pre>"},{"location":"guides/realsense-setup/#wsl2-side","title":"WSL2 Side","text":"<pre><code># Verify device is visible\nlsusb | grep Intel\n\n# May need permissions\nsudo chmod 666 /dev/video*\n</code></pre>"},{"location":"guides/realsense-setup/#persistent-attachment","title":"Persistent Attachment","text":"<p>Create a script to auto-attach on boot:</p> <pre><code># attach-realsense.ps1\nusbipd wsl attach --busid 2-3\n</code></pre>"},{"location":"guides/realsense-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/realsense-setup/#no-device-connected","title":"\"No device connected\"","text":"<ol> <li>Check USB connection</li> <li>Try different USB port (USB 3.0 required)</li> <li>WSL2: Verify usbipd attachment</li> </ol>"},{"location":"guides/realsense-setup/#poor-depth-quality","title":"Poor Depth Quality","text":"<ul> <li>Clean the IR sensors</li> <li>Ensure adequate lighting (not too bright/dark)</li> <li>Check for reflective surfaces</li> <li>Enable depth filtering (see above)</li> </ul>"},{"location":"guides/realsense-setup/#frame-drops","title":"Frame Drops","text":"<ul> <li>Reduce resolution or FPS</li> <li>Check USB bandwidth (avoid hubs)</li> <li>Close other applications using USB</li> </ul>"},{"location":"guides/realsense-setup/#next-steps","title":"Next Steps","text":"<ul> <li>RTAB-Map Setup \u2014 SLAM configuration</li> <li>Quick Start \u2014 Run RTSM</li> </ul>"},{"location":"guides/rtabmap-setup/","title":"RTAB-Map Setup","text":"<p>This guide covers setting up RTAB-Map to provide SLAM poses for RTSM.</p>"},{"location":"guides/rtabmap-setup/#overview","title":"Overview","text":"<p>RTSM needs camera poses (position + orientation) to project 2D detections into 3D space. RTAB-Map is a popular open-source visual SLAM system that works well with RGB-D cameras.</p> <pre><code>RealSense \u2192 RTAB-Map \u2192 Poses \u2192 ZeroMQ \u2192 RTSM\n</code></pre>"},{"location":"guides/rtabmap-setup/#installation","title":"Installation","text":""},{"location":"guides/rtabmap-setup/#ubuntu-wsl2","title":"Ubuntu / WSL2","text":"<pre><code>sudo apt update\nsudo apt install ros-humble-rtabmap-ros\n</code></pre> <p>Or build from source:</p> <pre><code>git clone https://github.com/introlab/rtabmap.git\ncd rtabmap/build\ncmake ..\nmake -j$(nproc)\nsudo make install\n</code></pre>"},{"location":"guides/rtabmap-setup/#windows","title":"Windows","text":"<p>Download from RTAB-Map releases.</p>"},{"location":"guides/rtabmap-setup/#running-with-realsense","title":"Running with RealSense","text":""},{"location":"guides/rtabmap-setup/#option-1-standalone-no-ros","title":"Option 1: Standalone (No ROS)","text":"<pre><code>rtabmap-realsense\n</code></pre> <p>This opens the RTAB-Map GUI with RealSense input.</p>"},{"location":"guides/rtabmap-setup/#option-2-ros-2","title":"Option 2: ROS 2","text":"<pre><code>ros2 launch rtabmap_launch rtabmap.launch.py \\\n  rgb_topic:=/camera/color/image_raw \\\n  depth_topic:=/camera/depth/image_rect_raw \\\n  camera_info_topic:=/camera/color/camera_info\n</code></pre>"},{"location":"guides/rtabmap-setup/#zeromq-bridge","title":"ZeroMQ Bridge","text":"<p>RTSM expects poses via ZeroMQ. Use the bridge utility:</p> <pre><code># From the rtsm-rtabmap-bridge repo\npython rtabmap_zmq_bridge.py --rtabmap-addr localhost:5555 --zmq-pub tcp://*:5556\n</code></pre>"},{"location":"guides/rtabmap-setup/#message-format","title":"Message Format","text":"<p>The bridge publishes pose messages:</p> <pre><code>{\n  \"timestamp\": 1705312200.123,\n  \"position\": [1.2, 0.4, 2.1],\n  \"orientation\": [0.0, 0.0, 0.0, 1.0],\n  \"frame_id\": 12345\n}\n</code></pre>"},{"location":"guides/rtabmap-setup/#configuration","title":"Configuration","text":""},{"location":"guides/rtabmap-setup/#rtab-map-parameters","title":"RTAB-Map Parameters","text":"<p>For indoor robotics, these defaults work well:</p> <pre><code>Rtabmap/DetectionRate=2\nVis/MinInliers=15\nRGBD/OptimizeMaxError=3.0\nMem/STMSize=30\n</code></pre>"},{"location":"guides/rtabmap-setup/#rtsm-configuration","title":"RTSM Configuration","text":"<p>In <code>config/rtsm.yaml</code>:</p> <pre><code>io:\n  zmq_slam_addr: \"tcp://localhost:5556\"\n</code></pre>"},{"location":"guides/rtabmap-setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/rtabmap-setup/#no-poses-received","title":"\"No poses received\"","text":"<ol> <li>Check RTAB-Map is running and tracking</li> <li>Verify ZMQ bridge is connected: <code>netstat -an | grep 5556</code></li> <li>Check for firewall blocking localhost ports</li> </ol>"},{"location":"guides/rtabmap-setup/#drift-poor-tracking","title":"Drift / Poor Tracking","text":"<ul> <li>Ensure adequate lighting</li> <li>Add visual features to the environment (avoid blank walls)</li> <li>Reduce camera motion speed</li> <li>Enable loop closure in RTAB-Map</li> </ul>"},{"location":"guides/rtabmap-setup/#wsl2-usb-issues","title":"WSL2 USB Issues","text":"<p>WSL2 doesn't natively support USB. Use usbipd-win:</p> <pre><code># PowerShell (admin)\nusbipd wsl list\nusbipd wsl attach --busid &lt;BUSID&gt;\n</code></pre>"},{"location":"guides/rtabmap-setup/#next-steps","title":"Next Steps","text":"<ul> <li>RealSense Setup \u2014 Camera configuration</li> <li>Configuration \u2014 RTSM settings</li> </ul>"}]}